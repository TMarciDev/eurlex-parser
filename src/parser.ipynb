{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from utils import html_table_to_markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n9102\\AppData\\Local\\Temp\\ipykernel_9264\\3452964181.py:9: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(response.text, 'lxml')\n"
     ]
    }
   ],
   "source": [
    "def get_data_by_celex_id(celex_id: str, language: str = \"en\") -> dict:\n",
    "    \"\"\"\n",
    "    Only support en for now\n",
    "    \"\"\"    \n",
    "\n",
    "    url = f\"https://eur-lex.europa.eu/legal-content/{language}/TXT/HTML/?uri=CELEX:{celex_id}\"    \n",
    "    response = requests.get(url)     \n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                       \n",
    "    return {\n",
    "        'title': parse_title(soup),\n",
    "        'preamble': parse_pbl(soup),        \n",
    "        'articles': parse_articles(soup),\n",
    "        'final_part': parse_fnp(soup),\n",
    "        'annexes': parse_annexes(soup)\n",
    "    }\n",
    "\n",
    "def parse_title(soup):\n",
    "    title_text = ''\n",
    "    tit_1_div = soup.find('div', id=\"tit_1\")\n",
    "    if tit_1_div:\n",
    "        title_text = tit_1_div.text\n",
    "        title_text = title_text.replace('\\u00a0',' ')\n",
    "    return title_text\n",
    "\n",
    "def parse_fnp(soup):\n",
    "    fnp_text = ''\n",
    "    fnp_1_div = soup.find('div', id=\"fnp_1\")\n",
    "    if fnp_1_div:\n",
    "        res = [line for line in fnp_1_div.text.split('\\n') if line.strip()]\n",
    "        res = '\\n'.join(res)\n",
    "        fnp_text = re.sub(r'(\\(\\d+\\))\\n', r'\\1 ', res)\n",
    "        fnp_text = fnp_text.replace('\\u00a0',' ')\n",
    "    return fnp_text\n",
    "\n",
    "def parse_pbl(soup):\n",
    "    pbl_text = ''\n",
    "    pbl_1_div = soup.find('div', id=\"pbl_1\") \n",
    "    if pbl_1_div:\n",
    "        res = [line for line in pbl_1_div.text.split('\\n') if line.strip()]\n",
    "        res = '\\n'.join(res)\n",
    "        pbl_text = re.sub(r'(\\(\\d+\\))\\n', r'\\1 ', res)\n",
    "        pbl_text = pbl_text.replace('\\u00a0',' ')\n",
    "\n",
    "    notes = extract_notes(soup, pbl_1_div) \n",
    "                \n",
    "    return {\n",
    "        'text': pbl_text,\n",
    "        'notes': notes\n",
    "    }\n",
    "        \n",
    "def parse_annexes(soup):\n",
    "    annexes = []\n",
    "    divs_with_anx_id = soup.find_all(\"div\", class_=\"eli-container\", id=lambda x: x and x.startswith(\"anx\"))\n",
    "    for div in divs_with_anx_id:\n",
    "        annex_data = {}      \n",
    "        annex_id = ''\n",
    "        annex_title = ''\n",
    "        annex_text = ''\n",
    "        annex_table = ''\n",
    "        for c in div.children:\n",
    "            if c.name == 'p' and \"doc-ti\" in str(c.get('class')):\n",
    "                annex_id = c.text.strip()\n",
    "            elif c.name == 'p' and \"ti-grseq-1\" in str(c.get('class')) and not annex_title:\n",
    "                annex_title = c.text.strip()\n",
    "            elif c.name == 'table' and \"table\" in str(c.get('class')):\n",
    "                annex_table = html_table_to_markdown(str(c))                \n",
    "            else:                                                \n",
    "                annex_text += clean_text(c.text)\n",
    "        \n",
    "        annex_text = annex_text.lstrip('\\n').rstrip('\\n').replace('\\n\\n\\n','\\n')\n",
    "        annex_data['id'] = annex_id\n",
    "        annex_data['title'] = annex_title\n",
    "        annex_data['text'] = annex_text\n",
    "        annex_data['table'] = annex_table\n",
    "        annexes.append(annex_data)\n",
    "    return annexes\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)                                                                                \n",
    "    text = re.sub(r'\\n([a-z0-9]\\))', r' \\1', text)\n",
    "    text = re.sub(r'\\n(\\d+\\.\\s+)', r' \\1', text)\n",
    "    text = re.sub(r'(\\(\\d+\\))\\n', r'\\1 ', text)\n",
    "    text = re.sub(r'(\\s*\\d+\\.)\\n', r'\\1 ', text)\n",
    "    text = re.sub(r'(\\([a-z]\\))\\n', r'\\1 ', text)\n",
    "    text = re.sub(r'(\\([IVXLCDM]+\\))\\n', r'\\1 ', text)\n",
    "    text = re.sub(r'(\\([ivxlcdm]+\\))\\n', r'\\1 ', text)\n",
    "    text = text.replace('\\n\\n','')\n",
    "    text = text.replace('\\u00a0',' ')\n",
    "    return text\n",
    "\n",
    "def find_parent_title(div, depth=0, results=None):\n",
    "        if results is None:\n",
    "            results = {}\n",
    "        if div is None or depth > 10:  # Avoid too deep recursion\n",
    "            return results\n",
    "        key, value = None, None\n",
    "        for d in div.children:\n",
    "            if d.name == 'p' and d.get('class') == ['oj-ti-section-1']:\n",
    "                key = d.text.strip()\n",
    "            elif d.name == 'div' and d.get('class') == ['eli-title']:\n",
    "                value = d.text.strip()\n",
    "        if key and value:\n",
    "            results[key] = value\n",
    "        parent_div = div.findParent(\"div\")\n",
    "        return find_parent_title(parent_div, depth + 1, results)\n",
    "\n",
    "def parse_articles(soup):\n",
    "    articles = []    \n",
    "    # bottom up\n",
    "    divs_with_art_id = soup.find_all(\"div\", class_=\"eli-subdivision\", id=lambda x: x and x.startswith(\"art\"))\n",
    "    for i, div in enumerate(divs_with_art_id):        \n",
    "        notes = extract_notes(soup, div)        \n",
    "        article_data = {}                \n",
    "        article_id = ''\n",
    "        article_title = ''\n",
    "        article_text = ''        \n",
    "        for c in div.children:\n",
    "            if c.name == 'p' and \"ti-art\" in str(c.get('class')):\n",
    "                article_id = c.text.replace(\"\\n\", \"\")\n",
    "            elif c.name == 'div' and c.get('class') == ['eli-title']:\n",
    "                article_title = c.text.replace(\"\\n\", \"\")                \n",
    "            # elif c.findChildren == 'p' and \"sti-art\" in str(c.get('class')):\n",
    "            #     article_title = c.text.replace(\"\\n\", \"\")\n",
    "            else:                                                \n",
    "                article_text += clean_text(c.text)\n",
    "        \n",
    "        article_text = article_text.lstrip('\\n').rstrip('\\n').replace('\\n\\n\\n','\\n')            \n",
    "                    \n",
    "        parent_info = find_parent_title(div.findParent(\"div\"))\n",
    "        article_data['id'] = article_id\n",
    "        article_data['title'] = article_title\n",
    "        article_data['text'] = article_text\n",
    "        article_data['metadata'] = parent_info\n",
    "        article_data['notes'] = notes\n",
    "        articles.append(article_data)\n",
    "    return articles\n",
    "\n",
    "def extract_note_text(text):\n",
    "    cleaned_text = text.strip().replace('\\xa0', '')\n",
    "    cleaned_text = re.sub(r'^\\(\\d+\\)\\s+', '', cleaned_text)    \n",
    "    cleaned_text = re.sub(r'^\\(\\d+\\)', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'^\\(\\*\\d+\\)', '', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def extract_notes(soup, div):\n",
    "    note_tags = div.find_all('span', class_='oj-super oj-note-tag') if div else []    \n",
    "    notes = []\n",
    "\n",
    "    for note in note_tags:\n",
    "        note_dic = {}          \n",
    "        \n",
    "        foot_note_id = note.findParent('a')['href'][1:]\n",
    "        foot_note = soup.find('a', id=foot_note_id)\n",
    "\n",
    "        note_dic['id'] = note.text\n",
    "        note_text = foot_note.findParent('p').text\n",
    "        cleaned_note_text = extract_note_text(note_text)\n",
    "        note_dic['text'] = cleaned_note_text\n",
    "        \n",
    "        url = ''\n",
    "        a_tags = foot_note.findParent('p').find_all('a')        \n",
    "        if len(a_tags) >= 2:\n",
    "            second_a_tag = a_tags[1]\n",
    "            href = second_a_tag.get('href')\n",
    "            index = href.find(\"legal-content\")\n",
    "            url = \"https://eur-lex.europa.eu/\" + href[index:]\n",
    "        note_dic['url'] = url     \n",
    "        notes.append(note_dic)\n",
    "    return notes\n",
    "\n",
    "\n",
    "def get_json_by_celex_id(celex_id) -> str:\n",
    "    data = get_data_by_celex_id(celex_id)\n",
    "    return json.dumps(data, indent=4)\n",
    "\n",
    "def get_articles_by_celex_id(celex_id) -> pd.DataFrame:\n",
    "    data = get_data_by_celex_id(celex_id)\n",
    "    articles = data['articles']\n",
    "    return pd.DataFrame(articles, columns=[\"id\", \"title\", \"text\", \"metadata\", \"notes\"])\n",
    "    \n",
    "    \n",
    "\n",
    "# data = get_data_by_celex_id('32013R0575')\n",
    "\n",
    "# df = get_articles_by_celex_id('32013R0575')\n",
    "# df.head(5)\n",
    "\n",
    "with open('test32019R0947.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(get_json_by_celex_id('32019R0947'))\n",
    "\n",
    "# with open('de.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(get_html_by_celex_id('32013R0575', 'de'), indent=4))\n",
    "\n",
    "# with open('en.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(get_data_by_celex_id('32013R0575'), indent=4))\n",
    "\n",
    "# get_html_by_celex_id('32013R0575', 'fr')\n",
    "\n",
    "\n",
    "# with open('32013L0036.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(get_html_by_celex_id('32013L0036'))\n",
    "\n",
    "# with open('32019L0878.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(get_html_by_celex_id('32019L0878'))\n",
    "\n",
    "# with open('32019R0876.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(get_html_by_celex_id('32019R0876'))\n",
    "\n",
    "\n",
    "# get_html_by_celex_id('32013R0575')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
